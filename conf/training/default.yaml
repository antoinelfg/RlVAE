# @package training

# Default training configuration
defaults:
  - _self_

# Training parameters
trainer:
  max_epochs: 100
  accelerator: "gpu"
  devices: 1  # Use single GPU to avoid device mismatches
  strategy: "auto"  # Use auto strategy for single GPU
  precision: "16-mixed"
  log_every_n_steps: 10
  val_check_interval: 1.0
  num_sanity_val_steps: 2
  enable_progress_bar: true
  enable_model_summary: true
  deterministic: false
  use_distributed_sampler: false  # Disable for single GPU

# Data parameters
data:
  batch_size: 16
  num_workers: 16
  pin_memory: true

# Model parameters
model:
  latent_dim: 32
  n_flows: 8
  beta: 1.0
  riemannian_beta: 0.1
  posterior:
    type: "riemannian_metric"
  sampling:
    method: "enhanced_riemannian"
    use_riemannian: true
  loop:
    mode: "closed"
    penalty: 0.1

# Optimization
optimizer:
  name: "adam"
  lr: 0.001
  weight_decay: 0.0001

# Logging
logging:
  log_every_n_steps: 10
  save_top_k: 3
  monitor: "val_loss"
  mode: "min"

# Visualization
visualization:
  frequency: 10
  level: "standard"

# Data splits
data_splits:
  train: 0.7
  val: 0.15
  test: 0.15

# Scheduler
scheduler:
  mode: "min"
  factor: 0.5
  patience: 10
  min_lr: 1e-7

# Early stopping
early_stopping:
  patience: 20
  monitor: "val_loss"
  mode: "min" 