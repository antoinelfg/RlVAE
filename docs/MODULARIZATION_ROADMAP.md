# RiemannianFlowVAE Modularization Roadmap

## ðŸŽ¯ **PHASE 1 COMPLETE âœ…**

### **Achievements**
- **MetricTensor Component**: Successfully extracted and modularized metric tensor computations
- **MetricLoader Component**: Created flexible loading system for pretrained metrics
- **Numerical Accuracy**: Perfect compatibility with original implementation (differences < 1e-18)
- **Performance**: 2x speedup over original implementation
- **Test Coverage**: Comprehensive test suite with benchmarks

### **Validation Results**
```
âœ… NUMERICAL ACCURACY: PASSED
âœ… G difference: 9.459e-19
âœ… G_inv difference: 0.000e+00
âœ… Performance: 2x speedup
âœ… Device handling: Fixed
âœ… Test coverage: Complete
```

---

## ðŸš€ **PHASE 2: SAMPLING STRATEGIES** (Next Priority)

### **Target Components**
```
src/models/samplers/
â”œâ”€â”€ base_sampler.py           # Abstract base class â­ NEXT
â”œâ”€â”€ riemannian_sampler.py     # Extract WorkingRiemannianSampler
â”œâ”€â”€ hmc_sampler.py            # Extract RiemannianHMCSampler
â”œâ”€â”€ rhvae_sampler.py          # Extract OfficialRHVAESampler
â””â”€â”€ sampler_factory.py       # Unified sampler creation
```

### **Implementation Steps**

#### **Step 2.1: Base Sampler Interface**
```python
# src/models/samplers/base_sampler.py
from abc import ABC, abstractmethod
import torch
from typing import Optional, Dict, Any

class BaseSampler(ABC):
    """Abstract base class for all sampling strategies."""
    
    def __init__(self, model, device: Optional[torch.device] = None):
        self.model = model
        self.device = device or torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    @abstractmethod
    def sample_posterior(self, mu: torch.Tensor, log_var: torch.Tensor) -> torch.Tensor:
        """Sample from posterior q(z|x)."""
        pass
    
    @abstractmethod
    def sample_prior(self, num_samples: int) -> torch.Tensor:
        """Sample from prior p(z)."""
        pass
    
    def get_config(self) -> Dict[str, Any]:
        """Get sampler configuration."""
        return {
            'sampler_type': self.__class__.__name__,
            'device': str(self.device),
        }
```

#### **Step 2.2: Extract WorkingRiemannianSampler** 
- Lines 61-434 from `riemannian_flow_vae.py`
- 374 lines of complex sampling logic
- Multiple sampling methods: enhanced, geodesic, basic
- Prior sampling strategies

#### **Step 2.3: Extract RiemannianHMCSampler**
- Lines 435-630 from `riemannian_flow_vae.py`
- 196 lines of HMC implementation
- Complex mathematical operations
- Temperature scheduling

#### **Step 2.4: Extract OfficialRHVAESampler**
- Lines 631-786 from `riemannian_flow_vae.py`
- 156 lines of RHVAE integration
- External dependency handling

### **Expected Benefits**
- **50% reduction** in main model complexity
- **Isolated testing** of sampling strategies
- **Easy experimentation** with new samplers
- **Plugin architecture** for research

---

## ðŸ”§ **PHASE 3: FLOW MANAGEMENT** 

### **Target Components**
```
src/models/components/
â”œâ”€â”€ flow_manager.py           # Flow sequence management
â”œâ”€â”€ flow_config.py            # Flow configuration
â””â”€â”€ temporal_dynamics.py     # Temporal evolution logic
```

### **Current Flow Logic Location**
Lines 1176-1190 in `forward()` method:
```python
# Propagate through flows (temporal evolution)
for t in range(1, n_obs):
    flow_res = self.flows[t-1](z_seq[-1])
    z_t = flow_res.out
    log_det = flow_res.log_abs_det_jac
    z_seq.append(z_t)
    log_det_sum += log_det
```

### **Target Architecture**
```python
class FlowManager(nn.Module):
    def __init__(self, config: FlowConfig):
        super().__init__()
        self.flows = self._create_flows(config)
    
    def propagate_sequence(self, z_0: torch.Tensor, n_steps: int) -> Tuple[torch.Tensor, torch.Tensor]:
        """Propagate latent through temporal flow sequence."""
        z_seq = [z_0]
        log_det_sum = torch.zeros(z_0.shape[0], device=z_0.device)
        
        for t in range(1, n_steps):
            flow_res = self.flows[t-1](z_seq[-1])
            z_seq.append(flow_res.out)
            log_det_sum += flow_res.log_abs_det_jac
            
        return torch.stack(z_seq, dim=1), log_det_sum
```

---

## ðŸ“Š **PHASE 4: LOSS MANAGEMENT**

### **Target Components**
```
src/models/components/
â”œâ”€â”€ loss_manager.py           # Loss computation coordination
â”œâ”€â”€ kl_losses.py             # KL divergence variants
â””â”€â”€ reconstruction_losses.py  # Reconstruction loss variants
```

### **Current Loss Logic Locations**
- Lines 1192-1229: Loss computation based on posterior type
- Lines 1005-1079: Riemannian metric KL loss
- Lines 1327-1395: Legacy Riemannian KL loss

### **Target Architecture**
```python
class LossManager:
    def __init__(self, config: LossConfig):
        self.config = config
        
    def compute_total_loss(
        self, 
        recon_x: torch.Tensor, 
        x: torch.Tensor,
        z_samples: torch.Tensor,
        mu: torch.Tensor, 
        log_var: torch.Tensor,
        log_det_jacobian: torch.Tensor,
        posterior_type: str,
        loop_mode: str
    ) -> Dict[str, torch.Tensor]:
        """Compute all loss components."""
        
        losses = {}
        
        # Reconstruction loss
        losses['recon_loss'] = self.compute_reconstruction_loss(recon_x, x, loop_mode)
        
        # KL divergence (depends on posterior type)  
        losses['kl_loss'] = self.compute_kl_loss(mu, log_var, z_samples, posterior_type)
        
        # Flow loss
        losses['flow_loss'] = -log_det_jacobian.mean()
        
        # Total loss
        losses['total_loss'] = (
            losses['recon_loss'] + 
            self.config.beta * losses['kl_loss'] + 
            losses['flow_loss']
        )
        
        return losses
```

---

## ðŸ—ï¸ **PHASE 5: CORE MODEL REFACTORING**

### **Target: Clean Main Model**
Reduce main `RiemannianFlowVAE` class from 609 lines to ~150 lines:

```python
class RiemannianFlowVAE(nn.Module):
    """Clean, modular Riemannian Flow VAE implementation."""
    
    def __init__(self, config: RiemannianVAEConfig):
        super().__init__()
        self.config = config
        
        # Core components
        self.encoder = self._create_encoder(config.encoder_config)
        self.decoder = self._create_decoder(config.decoder_config)
        
        # Modular components
        self.metric_tensor = MetricTensor(config.latent_dim)
        self.flow_manager = FlowManager(config.flow_config)
        self.sampler = SamplerFactory.create(config.sampler_config, self)
        self.loss_manager = LossManager(config.loss_config)
        
    def forward(self, x: torch.Tensor) -> ModelOutput:
        """Clean, focused forward pass."""
        # Encode
        mu, log_var = self.encode(x[:, 0])
        
        # Sample
        z_0 = self.sampler.sample_posterior(mu, log_var)
        
        # Propagate
        z_seq, log_det = self.flow_manager.propagate_sequence(z_0, x.shape[1])
        
        # Decode
        recon_x = self.decode(z_seq)
        
        # Compute losses
        losses = self.loss_manager.compute_total_loss(
            recon_x, x, z_0, mu, log_var, log_det, 
            self.config.posterior_type, self.config.loop_mode
        )
        
        return ModelOutput(recon_x=recon_x, z=z_seq, **losses)
        
    def load_pretrained_components(self, paths: Dict[str, str]):
        """Load all pretrained components."""
        if 'encoder' in paths:
            self.encoder.load_state_dict(torch.load(paths['encoder']))
        if 'decoder' in paths:
            self.decoder.load_state_dict(torch.load(paths['decoder']))
        if 'metric' in paths:
            metric_data = MetricLoader().load_from_file(paths['metric'])
            self.metric_tensor.load_pretrained(**metric_data)
```

---

## ðŸ“ˆ **PROJECTED BENEFITS**

### **Code Quality Improvements**
- **Complexity Reduction**: 60% reduction in cyclomatic complexity
- **Line Count**: Main model: 609 â†’ 150 lines (-75%)
- **Modularity**: 5 focused modules instead of 1 monolithic file
- **Testability**: 90%+ test coverage with isolated unit tests

### **Research Productivity**
- **Experiment Speed**: 3x faster new sampler implementation
- **A/B Testing**: Easy comparison of different components
- **Debugging**: Isolated component debugging
- **Collaboration**: Parallel development possible

### **Performance**
- **Memory**: Better memory management with focused components
- **Speed**: Current 2x speedup, expecting 3-5x overall
- **Optimization**: Targeted optimization of bottleneck components

### **Extensibility**
- **Plugin Architecture**: Easy addition of new samplers/metrics
- **Configuration**: YAML-driven model variants
- **Research**: Modular experimentation framework

---

## ðŸŽ¯ **IMMEDIATE NEXT STEPS**

### **Week 1: Sampler Extraction**
1. [ ] Create `base_sampler.py` with abstract interface
2. [ ] Extract `WorkingRiemannianSampler` to `riemannian_sampler.py`
3. [ ] Create comprehensive tests for sampler components
4. [ ] Validate numerical compatibility

### **Week 2: HMC and RHVAE Samplers**
1. [ ] Extract `RiemannianHMCSampler` to `hmc_sampler.py`
2. [ ] Extract `OfficialRHVAESampler` to `rhvae_sampler.py`
3. [ ] Create `SamplerFactory` for unified creation
4. [ ] Integration testing with existing metric components

### **Week 3: Flow Management**
1. [ ] Extract flow logic to `FlowManager`
2. [ ] Create `FlowConfig` for configuration
3. [ ] Test temporal dynamics in isolation
4. [ ] Performance benchmarking

### **Week 4: Loss Management**
1. [ ] Extract loss computations to `LossManager`
2. [ ] Modularize KL divergence variants
3. [ ] Test all loss combinations
4. [ ] Backward compatibility validation

### **Week 5: Core Refactoring**
1. [ ] Refactor main `RiemannianFlowVAE` class
2. [ ] Create unified configuration system
3. [ ] Comprehensive integration tests
4. [ ] Performance validation and optimization

---

## ðŸ† **SUCCESS METRICS**

### **Technical Metrics**
- [ ] **Code Coverage**: >90% with modular tests
- [ ] **Performance**: Maintain or improve training speed
- [ ] **Compatibility**: Perfect numerical accuracy (< 1e-15 difference)
- [ ] **Complexity**: 60% reduction in cyclomatic complexity per module

### **Research Metrics**
- [ ] **Experiment Setup**: 70% reduction in time to create new experiments
- [ ] **Component Reuse**: Ability to mix and match components across projects
- [ ] **Research Velocity**: 3x faster implementation of new ideas

### **Maintenance Metrics**
- [ ] **Bug Fix Time**: 50% reduction in average fix time
- [ ] **Feature Addition**: 60% reduction in time to add new features
- [ ] **Documentation**: Complete API documentation for all components

---

This roadmap builds on our **successful Phase 1** and provides a clear path to transform the monolithic RiemannianFlowVAE into a world-class modular research framework! ðŸš€ 